#!/usr/bin/env python
# -*- coding:utf-8 -*-
#需求：爬取糗事百科热图板块下的所有页面的糗图图片
import requests
import re
import os
if __name__=="__main__":
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.193 Safari/537.36'
    }
    #创建一个文件夹，保存所有的图片
    if not os.path.exists('./retuLibs'):
        os.mkdir('./retuLibs')
    #设置一个通用URL模板
    url = 'https://www.qiushibaike.com/imgrank/page/%d/'
    pageNum = 1
    for pageNum in range(1,3):
        new_url = format(url%pageNum)


        #使用通用爬虫对url对应的一整张页面进行爬取
        page_text = requests.get(url=new_url,headers=headers).text
        #使用聚焦爬虫将页面中所有的热图进行解析/提取

        ex = '<div class="thumb">.*?<img src="(.*?)" alt.*?</div>'
        img_src_list = re.findall(ex,page_text,re.S)
        #print(img_src_list)
        for src in img_src_list:
            #拼接出一个完整的图片url
            src = 'https:'+src
            #请求到了图片的二进制数据
            img_data = requests.get(url=src,headers=headers).content
            #生成图片名称
            img_name = src.split("/")[-1]
            #图片存储的路径
            imgPath = './retuLibs/'+img_name
            with open(imgPath,'wb') as fp:
                fp.write(img_data)
                print(img_name,"---------end---------")
